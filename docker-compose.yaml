version: "3.8"

services:
  # -----------------------------
  # Zookeeper (Kafka 依赖)
  # -----------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.1
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD-SHELL", "echo srvr | nc localhost 2181 || exit 1"]
      interval: 10s
      retries: 10
      timeout: 5s
      start_period: 10s

  # -----------------------------
  # Kafka
  # -----------------------------
  kafka:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      ALLOW_PLAINTEXT_LISTENER: "yes"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      retries: 10
      timeout: 10s
      start_period: 30s

  # -----------------------------
  # Spark Master
  # -----------------------------
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"  # Master UI
      - "7077:7077"  # 集群通信
      - "4040:4040"  # 应用 UI
    volumes:
      - spark-data:/tmp/spark
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 10s
      retries: 10
      timeout: 5s
      start_period: 30s

  # -----------------------------
  # Spark Worker
  # -----------------------------
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_PORT=8881
      - SPARK_WORKER_WEBUI_PORT=8082
    ports:
      - "8082:8082"  # Worker UI
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark-data:/tmp/spark
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8082 || exit 1"]
      interval: 10s
      retries: 10
      timeout: 5s
      start_period: 30s

  # -----------------------------
  # Flink JobManager
  # -----------------------------
  flink-jobmanager:
    image: flink:1.18.1
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    ports:
      - "8081:8081"  # Flink UI
    command: jobmanager
    volumes:
      - flink-data:/tmp/flink
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 10s
      retries: 10
      timeout: 5s
      start_period: 30s

  # -----------------------------
  # Flink TaskManager
  # -----------------------------
  flink-taskmanager:
    image: flink:1.18.1
    container_name: flink-taskmanager
    hostname: flink-taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
    command: taskmanager
    volumes:
      - flink-data:/tmp/flink

# -----------------------------
# 卷管理
# -----------------------------
volumes:
  spark-data:
  flink-data:
